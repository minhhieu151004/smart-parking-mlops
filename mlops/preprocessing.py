import argparse
import os
import pandas as pd
import numpy as np
import logging
import glob

# Cáº¥u hÃ¬nh Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Cáº¤U HÃŒNH ---
CAR_MAX = 100.0
HOUR_MAX = 24.0
N_STEPS = 288  # Sá»‘ bÆ°á»›c thá»i gian (5 phÃºt) trong 24 giá»

def create_sequences(data, n_steps):
    """Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u 2D thÃ nh 3D sequences cho LSTM/GRU."""
    X = []
    y = []
    # Dá»¯ liá»‡u: [car_count_scaled, hour_scaled]
    for i in range(len(data) - n_steps):
        # Input: n_steps quÃ¡ khá»©
        X.append(data[i : i + n_steps])
        # Output: BÆ°á»›c tiáº¿p theo (chá»‰ dá»± Ä‘oÃ¡n car_count - cá»™t index 0)
        y.append(data[i + n_steps, 0]) 
        
    return np.array(X), np.array(y)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # SageMaker sáº½ mount dá»¯ liá»‡u vÃ o cÃ¡c Ä‘Æ°á»ng dáº«n nÃ y
    parser.add_argument("--input-dir", type=str, default="/opt/ml/processing/input")
    parser.add_argument("--output-train-dir", type=str, default="/opt/ml/processing/train")
    parser.add_argument("--output-scaler-dir", type=str, default="/opt/ml/processing/scaler")
    
    args = parser.parse_args()

    logging.info("--- Báº®T Äáº¦U: PREPROCESSING (Láº¤Y 70% Dá»® LIá»†U CUá»I) ---")
    
    # 1. Äá»c Master File tá»« Input
    # Master file (parking_data.csv) Ä‘Æ°á»£c pipeline truyá»n vÃ o /opt/ml/processing/input
    all_files = glob.glob(os.path.join(args.input_dir, "*.csv"))
    
    if not all_files:
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file CSV nÃ o trong {args.input_dir}")
    
    input_file = all_files[0] # Láº¥y file Ä‘áº§u tiÃªn tÃ¬m tháº¥y
    logging.info(f"ðŸ“‚ Äang Ä‘á»c file: {input_file}")
    
    # Äá»c CSV (Master file format: DD/MM/YYYY)
    df = pd.read_csv(input_file)
    
    # 2. Xá»­ lÃ½ thá»i gian vÃ  Sáº¯p xáº¿p
    if 'timestamp' in df.columns:
        # dayfirst=True cá»±c ká»³ quan trá»ng vá»›i format DD/MM/YYYY
        df['timestamp'] = pd.to_datetime(df['timestamp'], dayfirst=True)
        df = df.sort_values('timestamp').reset_index(drop=True)
    else:
        raise ValueError("âŒ File dá»¯ liá»‡u thiáº¿u cá»™t 'timestamp'")
        
    logging.info(f"ðŸ“Š Tá»•ng dá»¯ liá»‡u gá»‘c: {len(df)} dÃ²ng. Tá»« {df['timestamp'].min()} Ä‘áº¿n {df['timestamp'].max()}")

    # 3. Láº¤Y 70% Dá»® LIá»†U CUá»I CÃ™NG (Má»šI NHáº¤T)
    train_ratio = 0.7
    total_rows = len(df)
    
    # TÃ­nh index cáº¯t: Láº¥y tá»« dÃ²ng thá»© (1 - 0.7) * total trá»Ÿ Ä‘i
    cut_index = int(total_rows * (1 - train_ratio))
    
    # Thá»±c hiá»‡n cáº¯t
    df_train = df.iloc[cut_index:].copy()
    
    logging.info(f"âœ‚ï¸ ÄÃ£ cáº¯t láº¥y 70% dá»¯ liá»‡u cuá»‘i: {len(df_train)} dÃ²ng.")
    logging.info(f"   -> Dá»¯ liá»‡u train tá»«: {df_train['timestamp'].min()} Ä‘áº¿n {df_train['timestamp'].max()}")

    # 4. Feature Engineering & Scaling (Thá»§ cÃ´ng)
    # LÃ½ do Scaling thá»§ cÃ´ng: Äá»ƒ khá»›p 100% vá»›i logic Inference trÃªn Lambda/Pi
    df_train['hour'] = df_train['timestamp'].dt.hour
    
    # Ã‰p kiá»ƒu float
    car_counts = df_train['car_count'].values.astype(float)
    hours = df_train['hour'].values.astype(float)
    
    # Scale
    car_counts_scaled = car_counts / CAR_MAX
    hours_scaled = hours / HOUR_MAX
    
    # Gá»™p láº¡i thÃ nh máº£ng 2D: [rows, 2] -> cá»™t 0: car, cá»™t 1: hour
    train_data_scaled = np.column_stack((car_counts_scaled, hours_scaled))
    
    # 5. Táº¡o Sequence (Sliding Window)
    if len(train_data_scaled) <= N_STEPS:
        raise ValueError(f"âŒ Dá»¯ liá»‡u sau khi cáº¯t ({len(train_data_scaled)}) Ã­t hÆ¡n N_STEPS ({N_STEPS}). KhÃ´ng thá»ƒ táº¡o sequence.")
        
    X_train, y_train = create_sequences(train_data_scaled, N_STEPS)
    
    logging.info(f"ðŸ“¦ KÃ­ch thÆ°á»›c táº­p Train: X={X_train.shape}, y={y_train.shape}")

    # 6. LÆ°u file .npy Ä‘á»ƒ bÆ°á»›c Train sá»­ dá»¥ng
    os.makedirs(args.output_train_dir, exist_ok=True)
    
    output_path = os.path.join(args.output_train_dir, "train_data.npy")
    # LÆ°u cáº£ X vÃ  y vÃ o 1 file cho gá»n, hoáº·c 2 file tuá»³ Ã½. á»ž Ä‘Ã¢y lÆ°u 1 file dictionary hoáº·c array
    # Äá»ƒ Ä‘Æ¡n giáº£n cho train_pipeline.py Ä‘á»c, ta lÆ°u dictionary
    np.save(output_path, {'X': X_train, 'y': y_train})
    
    logging.info(f"ðŸ’¾ ÄÃ£ lÆ°u file processed vÃ o: {output_path}")
    logging.info("--- HOÃ€N Táº¤T PREPROCESSING ---")