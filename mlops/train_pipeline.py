import argparse
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import logging
import json
import shutil
import sys

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def build_model(input_shape):
    """X√¢y d·ª±ng ki·∫øn tr√∫c m√¥ h√¨nh Hybrid (CNN-LSTM)."""
    model = Sequential([
        # --- CNN Block (Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng kh√¥ng gian/c·ª•c b·ªô) ---
        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        
        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        
        # Reshape ƒë·ªÉ ƒë∆∞a v√†o LSTM (Time steps gi·∫£m do Pooling)
        Reshape((-1, 128)),
        
        # --- LSTM Block (H·ªçc ph·ª• thu·ªôc chu·ªói th·ªùi gian d√†i) ---
        LSTM(units=150, return_sequences=True),
        Dropout(0.3),
        LSTM(units=100),
        Dropout(0.3),
        
        # --- Output Layer ---
        Dense(units=50, activation='relu'),
        Dense(units=1, activation='sigmoid')
    ])
    return model

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # --- Hyperparameters ---
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--learning-rate', type=float, default=0.001)
    parser.add_argument('--batch-size', type=int, default=64)
    
    # SageMaker Paths (T·ª± ƒë·ªông mount b·ªüi SageMaker)
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/opt/ml/model'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/train'))
    
    args = parser.parse_args()

    logging.info("--- B·∫ÆT ƒê·∫¶U TRAINING (RETRAIN FROM SCRATCH) ---")
    
    try:
        # 1. LOAD D·ªÆ LI·ªÜU ƒê√É PREPROCESS
        data_path = os.path.join(args.train, 'train_data.npy')
        
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {data_path}")
            
        logging.info(f"üìÇ ƒêang t·∫£i d·ªØ li·ªáu t·ª´ {data_path}...")
        # Load dictionary ch·ª©a X v√† y
        data = np.load(data_path, allow_pickle=True).item()
        X_all = data['X']
        y_all = data['y']
        
        logging.info(f"üì¶ T·ªïng d·ªØ li·ªáu: X shape: {X_all.shape}, y shape: {y_all.shape}")
        
        # 2. CHIA DATA TRAIN/VAL
        # ƒê∆∞a d·ªØ li·ªáu m·ªõi nh·∫•t (Drift) v√†o t·∫≠p Train.
        # Valid: L·∫•y 10% d·ªØ li·ªáu n·∫±m tr∆∞·ªõc ƒëo·∫°n m·ªõi nh·∫•t.
        
        total_len = len(X_all)
        valid_len = int(total_len * 0.1) # 10% Validation
        
        # ƒêo·∫°n Valid: T·ª´ 80% -> 90% (Ch·ª´a l·∫°i 10% cu·ªëi c√πng cho Train)
        valid_start = int(total_len * 0.8)
        valid_end = int(total_len * 0.9)
        
        # T√°ch Validation
        X_val = X_all[valid_start : valid_end]
        y_val = y_all[valid_start : valid_end]
        
        # T√°ch Training: (ƒê·∫ßu -> 80%) + (90% -> Cu·ªëi)
        # np.concatenate gi√∫p n·ªëi 2 ph·∫ßn l·∫°i
        X_train = np.concatenate((X_all[:valid_start], X_all[valid_end:]), axis=0)
        y_train = np.concatenate((y_all[:valid_start], y_all[valid_end:]), axis=0)
        
        logging.info(f"   -> Train size: {len(X_train)} (G·ªìm 10% d·ªØ li·ªáu m·ªõi nh·∫•t)")
        logging.info(f"   -> Valid size: {len(X_val)}")

        # 3. BUILD MODEL & TRAINING
        input_shape = (X_train.shape[1], X_train.shape[2]) # (288, 2)
        model = build_model(input_shape)
        
        model.compile(optimizer=Adam(learning_rate=args.learning_rate), loss='mean_squared_error')
        logging.info("‚úÖ Ki·∫øn tr√∫c Model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")

        # Checkpoint: Ch·ªâ l∆∞u model c√≥ val_loss t·ªët nh·∫•t
        checkpoint_path = os.path.join(args.model_dir, 'best_model_checkpoint.h5')
        
        callbacks = [
            ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
        ]

        logging.info("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh Fit...")
        history = model.fit(
            X_train, y_train,
            epochs=args.epochs,
            batch_size=args.batch_size,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=2
        )
        
        # 4. L∆ØU ARTIFACTS (CHO INFERENCE)
        logging.info(f"üíæ ƒêang l∆∞u model artifacts v√†o {args.model_dir}...")

        # A. L∆∞u Model Format SavedModel (Standard cho TF Serving/SageMaker)
        export_path = os.path.join(args.model_dir, '1')
        model.save(export_path) 
        logging.info(f"‚úÖ ƒê√£ l∆∞u TensorFlow SavedModel v√†o {export_path}")
        
        # B. L∆∞u Metrics Training
        final_train_loss = history.history['loss'][-1]
        final_val_loss = history.history['val_loss'][-1]
        
        # C. COPY INFERENCE CODE 
        # SageMaker Endpoint c·∫ßn file inference.py ƒë·ªÉ bi·∫øt c√°ch x·ª≠ l√Ω input/output
        current_dir = os.path.dirname(os.path.realpath(__file__))
        code_output_dir = os.path.join(args.model_dir, "code")
        os.makedirs(code_output_dir, exist_ok=True)
        
        # C√°c file ngu·ªìn n·∫±m c√πng th∆∞ m·ª•c v·ªõi script n√†y
        inference_src = os.path.join(current_dir, "inference.py") 
        requirements_src = os.path.join(current_dir, "requirements.txt")
        
        if os.path.exists(inference_src):
            shutil.copy(inference_src, os.path.join(code_output_dir, "inference.py"))
            logging.info("‚úÖ ƒê√£ copy inference.py")
        else:
            logging.warning("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y inference.py! Endpoint c√≥ th·ªÉ b·ªã l·ªói.")

        if os.path.exists(requirements_src):
            shutil.copy(requirements_src, os.path.join(code_output_dir, "requirements.txt"))
            logging.info("‚úÖ ƒê√£ copy requirements.txt")

        logging.info("--- TRAINING HO√ÄN T·∫§T TH√ÄNH C√îNG ---")

    except Exception as e:
        logging.error(f"‚ùå Training Th·∫•t B·∫°i: {e}", exc_info=True)
        sys.exit(1)