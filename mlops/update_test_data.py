import argparse
import os
import pandas as pd
import glob
import logging

# C·∫•u h√¨nh Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--drift-data-dir", type=str, default="/opt/ml/processing/drift_data")
    parser.add_argument("--original-test-dir", type=str, default="/opt/ml/processing/original_test")
    parser.add_argument("--output-dir", type=str, default="/opt/ml/processing/output")
    
    args = parser.parse_args()

    logging.info("--- B·∫ÆT ƒê·∫¶U: C·∫¨P NH·∫¨T T·∫¨P TEST ---")

    # 1. Load D·ªØ li·ªáu Drift (D·ªØ li·ªáu ng√†y m·ªõi nh·∫•t)
    # T√¨m file CSV trong th∆∞ m·ª•c drift_data
    drift_files = glob.glob(os.path.join(args.drift_data_dir, "*.csv"))
    if not drift_files:
        logging.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Drift. S·∫Ω ch·ªâ d√πng t·∫≠p test g·ªëc.")
        df_drift = pd.DataFrame()
    else:
        logging.info(f"ƒê·ªçc d·ªØ li·ªáu Drift t·ª´: {drift_files[0]}")
        df_drift = pd.read_csv(drift_files[0])

    # 2. Load T·∫≠p Test G·ªëc
    # T√¨m file CSV trong th∆∞ m·ª•c original_test
    test_files = glob.glob(os.path.join(args.original_test_dir, "*.csv"))
    if not test_files:
        logging.error("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·∫≠p Test g·ªëc tr√™n S3.")
        # Tr∆∞·ªùng h·ª£p n√†y critical, nh∆∞ng ta s·∫Ω handle m·ªÅm ƒë·ªÉ ko crash pipeline n·∫øu test
        df_original = pd.DataFrame()
    else:
        logging.info(f"ƒê·ªçc t·∫≠p Test g·ªëc t·ª´: {test_files[0]}")
        df_original = pd.read_csv(test_files[0])

    # 3. N·ªëi d·ªØ li·ªáu (Concatenate)
    if not df_drift.empty and not df_original.empty:
        # ƒê·∫£m b·∫£o format timestamp th·ªëng nh·∫•t ƒë·ªÉ sort
        if 'timestamp' in df_drift.columns:
            df_drift['timestamp'] = pd.to_datetime(df_drift['timestamp'], dayfirst=True, errors='coerce')
        if 'timestamp' in df_original.columns:
            df_original['timestamp'] = pd.to_datetime(df_original['timestamp'], dayfirst=True, errors='coerce')

        # N·ªëi: T·∫≠p g·ªëc + D·ªØ li·ªáu m·ªõi
        df_updated = pd.concat([df_original, df_drift], ignore_index=True)
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p (n·∫øu c√≥) v√† S·∫Øp x·∫øp l·∫°i
        df_updated = df_updated.drop_duplicates(subset=['timestamp'], keep='last')
        df_updated = df_updated.sort_values('timestamp')
        
        logging.info(f"‚úÖ ƒê√£ n·ªëi th√†nh c√¥ng. K√≠ch th∆∞·ªõc c≈©: {len(df_original)}, M·ªõi: {len(df_updated)}")
    
    elif not df_original.empty:
        logging.info("Ch·ªâ s·ª≠ d·ª•ng t·∫≠p Test g·ªëc (kh√¥ng c√≥ drift data).")
        df_updated = df_original
    else:
        logging.warning("C·∫£ t·∫≠p test g·ªëc v√† drift data ƒë·ªÅu r·ªóng/thi·∫øu!")
        df_updated = pd.DataFrame(columns=['car_count', 'timestamp'])

    # 4. L∆∞u k·∫øt qu·∫£
    os.makedirs(args.output_dir, exist_ok=True)
    output_path = os.path.join(args.output_dir, "updated_test_set.csv")
    
    # Format l·∫°i timestamp v·ªÅ string chu·∫©n tr∆∞·ªõc khi l∆∞u
    if 'timestamp' in df_updated.columns:
        df_updated['timestamp'] = df_updated['timestamp'].dt.strftime('%d/%m/%Y %H:%M:%S')

    # Ch·ªâ gi·ªØ c√°c c·ªôt quan tr·ªçng
    cols_to_save = ['car_count', 'timestamp']
    # N·∫øu c√≥ c·ªôt 'hour', gi·ªØ l·∫°i c≈©ng ƒë∆∞·ª£c, nh∆∞ng t·ªëi thi·ªÉu ph·∫£i c√≥ 2 c·ªôt tr√™n
    df_updated = df_updated[cols_to_save] if set(cols_to_save).issubset(df_updated.columns) else df_updated

    df_updated.to_csv(output_path, index=False)
    logging.info(f"üíæ ƒê√£ l∆∞u T·∫≠p Test M·ªõi v√†o: {output_path}")

    logging.info("--- HO√ÄN T·∫§T C·∫¨P NH·∫¨T T·∫¨P TEST ---")