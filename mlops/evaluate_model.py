import json
import os
import tarfile
import argparse
import logging
import shutil
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
from sklearn.metrics import mean_absolute_error, mean_squared_error

# C·∫•u h√¨nh Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# --- C·∫§U H√åNH ---
N_STEPS = 288           # ƒê·ªô d√†i chu·ªói ƒë·∫ßu v√†o (24h)
TIME_STEP_MINUTES = 5   # B∆∞·ªõc th·ªùi gian
FUTURE_STEP = 12        # D·ª± ƒëo√°n cho 60 ph√∫t sau (12 b∆∞·ªõc * 5p)
CAR_MAX = 100.0         # D√πng ƒë·ªÉ Inverse Scaling th·ªß c√¥ng 

def extract_model_artifact(model_tar_path, extract_dir):
    """Gi·∫£i n√©n model.tar.gz ra th∆∞ m·ª•c ƒë√≠ch"""
    if not os.path.exists(model_tar_path):
        logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file model t·∫°i: {model_tar_path}")
        return False
    
    logger.info(f"ƒêang gi·∫£i n√©n {model_tar_path} v√†o {extract_dir}...")
    try:
        with tarfile.open(model_tar_path, "r:gz") as tar:
            tar.extractall(path=extract_dir)
        return True
    except Exception as e:
        logger.error(f"L·ªói gi·∫£i n√©n: {e}")
        return False

def preprocess_test_csv(df, scaler_car, scaler_hour):
    try:
        # 1. Parse Timestamp
        if 'timestamp' in df.columns:
            # File CSV th∆∞·ªùng l∆∞u DD/MM/YYYY ho·∫∑c ISO. Th·ª≠ dayfirst=True tr∆∞·ªõc.
            df['timestamp'] = pd.to_datetime(df['timestamp'], dayfirst=True, errors='coerce')
            df = df.dropna(subset=['timestamp'])
            df = df.set_index('timestamp').sort_index()
        
        # 2. Resample 
        df_resampled = df.resample(f'{TIME_STEP_MINUTES}T').mean().interpolate(method='time')
        
        # Feature Engineering
        df_resampled['hour'] = df_resampled.index.hour
        
        # 3. Scaling 
        # √âp ki·ªÉu float ƒë·ªÉ tr√°nh l·ªói
        df_resampled['car_count'] = df_resampled['car_count'].astype(float)
        df_resampled['hour'] = df_resampled['hour'].astype(float)

        df_resampled['car_count_scaled'] = scaler_car.transform(df_resampled[['car_count']])
        df_resampled['hour_scaled'] = scaler_hour.transform(df_resampled[['hour']])
        
        # 4. T·∫°o Sequence
        # D·ªØ li·ªáu ƒë·∫ßu v√†o cho model: [car_scaled, hour_scaled]
        data_matrix = df_resampled[['car_count_scaled', 'hour_scaled']].values
        # D·ªØ li·ªáu g·ªëc ƒë·ªÉ so s√°nh (Ground Truth)
        raw_car_counts = df_resampled['car_count'].values
        
        X, y_true = [], []
        
        # Logic Sliding Window:
        # Input (X): t·ª´ i -> i + n_steps
        # Output (y): t·∫°i i + n_steps + future_step
        limit = len(data_matrix) - N_STEPS - FUTURE_STEP
        
        if limit <= 0:
            logger.error(f"D·ªØ li·ªáu test qu√° ng·∫Øn ({len(data_matrix)} d√≤ng). C·∫ßn t·ªëi thi·ªÉu {N_STEPS + FUTURE_STEP} d√≤ng.")
            return None, None

        for i in range(limit):
            X.append(data_matrix[i : i + N_STEPS])
            y_true.append(raw_car_counts[i + N_STEPS + FUTURE_STEP]) 
            
        return np.array(X), np.array(y_true)

    except Exception as e:
        logger.error(f"L·ªói Preprocessing Test Data: {e}")
        return None, None

def evaluate_single_model(model_extract_dir, df_test_raw, model_name="Model"):
    """
    Load model + scaler, preprocess test data v√† ƒë√°nh gi√°.
    """
    try:
        # 1. Load Scalers 
        scaler_car_path = os.path.join(model_extract_dir, "scaler_car_count.pkl")
        scaler_hour_path = os.path.join(model_extract_dir, "scaler_hour.pkl")
        
        if not os.path.exists(scaler_car_path) or not os.path.exists(scaler_hour_path):
            logger.error(f"[{model_name}] Thi·∫øu file Scaler (.pkl) trong artifact model!")
            return None
            
        scaler_car = joblib.load(scaler_car_path)
        scaler_hour = joblib.load(scaler_hour_path)
        
        # 2. Preprocess Data (T·∫°o X_test, y_test ngay t·∫°i ƒë√¢y)
        logger.info(f"[{model_name}] Preprocessing test CSV...")
        X_test, y_true = preprocess_test_csv(df_test_raw.copy(), scaler_car, scaler_hour)
        
        if X_test is None: return None
        logger.info(f"[{model_name}] Test Set Size: {len(X_test)} m·∫´u")

        # 3. Load Keras Model
        model_path = os.path.join(model_extract_dir, "1")
        if not os.path.exists(model_path):
             logger.error(f"[{model_name}] Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c model/1")
             return None
             
        logger.info(f"[{model_name}] Loading model...")
        model = tf.keras.models.load_model(model_path)

        # 4. Predict
        logger.info(f"[{model_name}] Predicting...")
        y_pred_scaled = model.predict(X_test, verbose=0)
        
        # 5. Inverse Transform (ƒê∆∞a d·ª± ƒëo√°n v·ªÅ s·ªë xe th·ª±c t·∫ø)
        # Model output shape (N, 1) -> Inverse b·∫±ng scaler_car
        y_pred_actual = scaler_car.inverse_transform(y_pred_scaled)
        
        # Flatten v·ªÅ m·∫£ng 1 chi·ªÅu ƒë·ªÉ so s√°nh
        y_pred_actual = y_pred_actual.flatten()
        y_true = y_true.flatten()
        
        # 6. T√≠nh Metrics
        mae = mean_absolute_error(y_true, y_pred_actual)
        mse = mean_squared_error(y_true, y_pred_actual)
        
        logger.info(f"[{model_name}] Result -> MAE: {mae:.4f} xe, MSE: {mse:.4f}")
        return {"mae": mae, "mse": mse}

    except Exception as e:
        logger.error(f"L·ªói khi ƒë√°nh gi√° {model_name}: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # C√°c tham s·ªë n√†y ƒë∆∞·ª£c truy·ªÅn t·ª´ build_pipeline.py
    parser.add_argument("--new-model-tar", type=str, default="/opt/ml/processing/new_model/model.tar.gz")
    parser.add_argument("--old-model-tar", type=str, default="/opt/ml/processing/old_model/model.tar.gz")
    parser.add_argument("--test-data", type=str, default="/opt/ml/processing/test/parking_test.csv")
    parser.add_argument("--output-dir", type=str, default="/opt/ml/processing/output")
    
    args = parser.parse_args()

    # 1. ƒê·ªçc File CSV Test G·ªëc
    logger.info(f"üìÇ ƒêang ƒë·ªçc file CSV Test t·ª´: {args.test_data}")
    try:
        if os.path.isdir(args.test_data):
            csv_files = [f for f in os.listdir(args.test_data) if f.endswith('.csv')]
            if csv_files:
                test_file_path = os.path.join(args.test_data, csv_files[0])
            else:
                raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file .csv trong th∆∞ m·ª•c test input")
        else:
            test_file_path = args.test_data

        df_test_raw = pd.read_csv(test_file_path)
        logger.info(f"üì¶ Raw Test Data: {len(df_test_raw)} d√≤ng.")
    except Exception as e:
        logger.error(f"‚ùå L·ªói ƒë·ªçc file Test: {e}")
        exit(1)

    # 2. ƒê√°nh gi√° MODEL M·ªöI (Candidate)
    new_model_dir = "/tmp/new_model"
    os.makedirs(new_model_dir, exist_ok=True)
    
    metrics_new = None
    if extract_model_artifact(args.new_model_tar, new_model_dir):
        # Truy·ªÅn df_test_raw v√†o, h√†m s·∫Ω t·ª± preprocess b·∫±ng scaler C·ª¶A MODEL M·ªöI
        metrics_new = evaluate_single_model(new_model_dir, df_test_raw, "NEW_MODEL")
    
    # 3. ƒê√°nh gi√° MODEL C≈® (Production)
    metrics_old = None
    old_model_dir = "/tmp/old_model"
    os.makedirs(old_model_dir, exist_ok=True)
    
    if os.path.exists(args.old_model_tar):
        if extract_model_artifact(args.old_model_tar, old_model_dir):
            # Truy·ªÅn df_test_raw v√†o, h√†m s·∫Ω t·ª± preprocess b·∫±ng scaler C·ª¶A MODEL C≈®
            metrics_old = evaluate_single_model(old_model_dir, df_test_raw, "OLD_MODEL")
    else:
        logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y model c≈© (L·∫ßn ch·∫°y ƒë·∫ßu ti√™n?).")
        metrics_old = {"mae": 9999.0, "mse": 9999.0}

    # 4. So s√°nh v√† T·∫°o B√°o c√°o
    mae_new = metrics_new["mae"] if metrics_new else 9999.0
    mae_old = metrics_old["mae"] if metrics_old else 9999.0
    
    comparison_result = "UNKNOWN"
    if mae_new < mae_old:
        comparison_result = "BETTER"
        logger.info(f"üöÄ Model M·ªõi T·ªêT H∆†N ({mae_new:.2f} vs {mae_old:.2f})")
    else:
        comparison_result = "WORSE"
        logger.info(f"üìâ Model M·ªõi T·ªÜ H∆†N ({mae_new:.2f} vs {mae_old:.2f})")

    # JSON Report
    report = {
        "regression_metrics": {
            "mae": {
                "value": mae_new,
                "standard_deviation": 0.0
            },
            "mse": {
                "value": metrics_new["mse"] if metrics_new else 9999.0,
                "standard_deviation": 0.0
            }
        },
        "comparison": {
            "new_mae": mae_new,
            "old_mae": mae_old,
            "result": comparison_result
        }
    }

    # 5. L∆∞u Output
    os.makedirs(args.output_dir, exist_ok=True)
    output_path = os.path.join(args.output_dir, "evaluation.json")
    
    with open(output_path, "w") as f:
        json.dump(report, f, indent=4)
        
    logger.info(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o ƒë√°nh gi√° v√†o: {output_path}")