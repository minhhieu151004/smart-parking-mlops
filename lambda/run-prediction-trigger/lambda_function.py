import boto3
import json
import os
import pandas as pd
import numpy as np
import joblib
from boto3.dynamodb.conditions import Key
from datetime import datetime, timedelta
from decimal import Decimal 
import tempfile
import sys

# --- C·∫§U H√åNH ---
ENDPOINT_NAME = os.environ.get('SAGEMAKER_ENDPOINT_NAME')
TABLE_RAW = 'SmartParkingRawData'
TABLE_PRED = 'SmartParkingPredictions'
SENSOR_ID = 'camera-01' 
S3_BUCKET = os.environ.get('S3_BUCKET', 'kltn-smart-parking-data') 

# --- ƒê∆Ø·ªúNG D·∫™N SCALER ---
SCALER_PREFIX = 'models/production' 
SCALER_KEY_CAR = f'{SCALER_PREFIX}/scaler_car_count.pkl' 
SCALER_KEY_HOUR = f'{SCALER_PREFIX}/scaler_hour.pkl'

# H·∫±ng s·ªë m√¥ h√¨nh
N_STEPS = 288 
TIME_STEP_MINUTES = 5 
PREDICTION_WINDOW_MINUTES = 60 

# --- KH·ªûI T·∫†O GLOBAL & CACHING ---
SCALER_ARTIFACTS = {} 

dynamodb = boto3.resource('dynamodb')
table_raw = dynamodb.Table(TABLE_RAW)
table_pred = dynamodb.Table(TABLE_PRED)
runtime = boto3.client('sagemaker-runtime')
s3_client = boto3.client('s3')

# --- H√ÄM T·∫¢I SCALERS ---
def load_scalers_from_s3():
    """T·∫£i scaler t·ª´ S3 v√† load ch√∫ng v√†o b·ªô nh·ªõ."""
    global SCALER_ARTIFACTS
    
    if SCALER_ARTIFACTS:
        return SCALER_ARTIFACTS

    temp_dir = tempfile.gettempdir()
    local_car_path = os.path.join(temp_dir, 'scaler_car.pkl')
    local_hour_path = os.path.join(temp_dir, 'scaler_hour.pkl')

    try:
        s3_client.download_file(S3_BUCKET, SCALER_KEY_CAR, local_car_path)
        s3_client.download_file(S3_BUCKET, SCALER_KEY_HOUR, local_hour_path)
        
        SCALER_ARTIFACTS = {
            'scaler_car': joblib.load(local_car_path),
            'scaler_hour': joblib.load(local_car_path)
        }
        return SCALER_ARTIFACTS
    
    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫£i ho·∫∑c load scalers t·ª´ S3. {e}")
        raise RuntimeError("Missing model artifacts for scaling.")

# --- H√ÄM TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ---
def _preprocess_and_scale(df, artifacts, n_steps=N_STEPS):
    df['car_count'] = pd.to_numeric(df['car_count'], errors='coerce').astype(float)
    df['timestamp'] = pd.to_datetime(df['timestamp'], dayfirst=True, errors='coerce')
    df = df.dropna(subset=['car_count', 'timestamp'])
    
    if len(df) < n_steps:
         raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu, c·∫ßn {n_steps} ƒëi·ªÉm.")

    scaler_car = artifacts['scaler_car']
    scaler_hour = artifacts['scaler_hour']

    df = df.set_index('timestamp').sort_index()
    df_resampled = df.resample(f'{TIME_STEP_MINUTES}T').mean().interpolate(method='time')
    
    df_resampled['hour'] = df_resampled.index.hour
    df_resampled['car_count_scaled'] = scaler_car.transform(df_resampled[['car_count']])
    df_resampled['hour_scaled'] = scaler_hour.transform(df_resampled[['hour']])
    
    sequence = df_resampled[['car_count_scaled', 'hour_scaled']].values[-n_steps:]
    
    return sequence.reshape(1, n_steps, 2), df_resampled.index[-1]


def lambda_handler(event, context):
    try:
        # --- B∆Ø·ªöC 1: TI·∫æP NH·∫¨N D·ªÆ LI·ªÜU TH√î T·ª™ PI5 (API Gateway) ---
        print("üîó ƒêang x·ª≠ l√Ω HTTP POST t·ª´ Pi5...")
        
        # 1. Parse body (D·ªØ li·ªáu Pi g·ª≠i l√™n: {"car_count": 45, "timestamp": "dd/mm/yyyy HH:MM:SS"})
        request_data = json.loads(event['body'])
        pi_timestamp_str = request_data['timestamp']
        pi_car_count = request_data['car_count']
        
        # 2. Chu·∫©n h√≥a Timestamp Pi g·ª≠i sang ISO ƒë·ªÉ l∆∞u DB
        pi_timestamp_dt = datetime.strptime(pi_timestamp_str, '%d/%m/%Y %H:%M:%S')
        iso_timestamp = pi_timestamp_dt.isoformat()
        
        # 3. Ghi d·ªØ li·ªáu Pi v·ª´a g·ª≠i v√†o b·∫£ng Raw Data (DB Write 1)
        table_raw.put_item(
            Item={
                'sensor_id': SENSOR_ID,
                'timestamp': iso_timestamp,
                'car_count': Decimal(str(pi_car_count)) 
            }
        )
        print(f"‚úÖ Ghi d·ªØ li·ªáu Pi v√†o DB th√†nh c√¥ng: {pi_car_count} xe.")
        
        # --- B∆Ø·ªöC 2: L·∫§Y L·ªäCH S·ª¨ V√Ä G·ªåI ENDPOINT ---
        
        # T·∫£i scalers
        artifacts = load_scalers_from_s3()
        
        # 1. L·∫•y 288 d√≤ng l·ªãch s·ª≠ (DB Read)
        response = table_raw.query(
            KeyConditionExpression=Key('sensor_id').eq(SENSOR_ID),
            Limit=N_STEPS, ScanIndexForward=False 
        )
        items = response['Items']
        
        if len(items) < N_STEPS:
            # ƒê√¢y l√† l·∫ßn kh·ªüi ƒë·ªông h·ªá th·ªëng, ch∆∞a ƒë·ªß 24h d·ªØ li·ªáu
            return {'statusCode': 202, 'body': json.dumps({"status": "COLD_START", "message": "Collecting more data..."})}

        items.reverse()
        df = pd.DataFrame(items)
        
        # 2. Ti·ªÅn x·ª≠ l√Ω (T·∫°o Tensor chu·∫©n [1, 288, 2])
        input_tensor, last_valid_ts = _preprocess_and_scale(df, artifacts) 
        
        # 3. L·∫•y Timestamp cho d·ª± ƒëo√°n
        pred_ts = last_valid_ts.floor(f'{TIME_STEP_MINUTES}min') + timedelta(minutes=PREDICTION_WINDOW_MINUTES)
        
        # 4. G√≥i v√†o format "instances"
        payload_data = {"instances": input_tensor.tolist()}
        json_payload = json.dumps(payload_data)
        
        # 5. G·ªçi SageMaker Endpoint
        response = runtime.invoke_endpoint(
            EndpointName=ENDPOINT_NAME,
            ContentType='application/json',
            Body=json_payload
        )
        
        result = json.loads(response['Body'].read().decode())
        
        # 6. H·∫≠u x·ª≠ l√Ω (Inverse Transform)
        scaled_pred_value = result['predictions'][0][0] 
        actual_pred_value = artifacts['scaler_car'].inverse_transform([[scaled_pred_value]])[0][0]
        final_prediction = int(round(actual_pred_value))

        # 7. L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n (DB Write 2)
        table_pred.put_item(
            Item={
                'sensor_id': SENSOR_ID,
                'timestamp': last_valid_ts.isoformat(),
                'prediction': final_prediction,
                'prediction_for': pred_ts.isoformat(), 
                'created_at': datetime.now().isoformat()
            }
        )
        
        # --- B∆Ø·ªöC 3: TR·∫¢ V·ªÄ PH·∫¢N H·ªíI CHO PI5 ---
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({
                "prediction": final_prediction, 
                "timestamp_for": pred_ts.strftime('%Y-%m-%d %H:%M:%S')
            })
        }

    except Exception as e:
        print(f"‚ùå L·ªñI KH√îNG X·ª¨ L√ù: {e}")
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({"error": str(e)})
        }